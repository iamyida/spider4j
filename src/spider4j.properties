#是否开启爬虫的中断/异常崩溃后重启恢复功能，默认不开启。【注意】若开启此功能，则爬虫爬取速度可能会有所降低
spider.enable_resume=false
#爬虫最大爬取深度，-1表示无限制
spider.max_depth=-1
#爬虫抓取网页失败后最大重试次数,默认重试3次
spider.page_failur_retry_times=3
#爬虫计划提取页面的最大个数，-1表示无限制
spider.max_pages_to_fetch=-1
#每个页面提取的最大URL个数设置，默认5000个
spider.max_outlinks_per_page=5000
#设置爬虫是否遵守robots协议，默认设置为遵守
spider.obey_robotstxt=true
#robots文件默认Map映射key-value键值对最大个数
spider.robots_map_max_size=200
#设置是否允许爬虫提取诸如图片、音频、视频、CSS、JS等二进制文件数据，默认设置为不允许
spider.allow_fetch_binary_content=false
#是否处理二进制数据，比如使用Tika
spider.process_binary_content_in_crawling=false
#设置提取的页面内容的默认编码
spider.page_default_encoding=UTF-8
#爬虫提取页面的最大字节数，单位:byte。【注意】超过此设置的页面将会被丢弃
spider.page_max_size=1048576
#爬虫模拟的浏览器用户代理，默认设置为Firefox
spider.user_agent=Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.65 Safari/537.36
#爬虫前后相邻两次请求的最小时间间隔，单位毫秒，默认为200毫秒。主要是为了避免爬取过于频繁而被对方服务器察觉，导致被封IP
spider.default_politeness_delay=200
#设置爬虫是否支持https协议，默认设置为支持
spider.support_https=true
#设置爬虫是否支持自动重定向，默认支持
spider.auto_follow_redirects=true
#是否在线更新顶级域名
spider.online_tld_list_update=true
#队列为空时是否关闭爬虫
spider.shutdown_on_empty_queue=true
#是否开启Http GZip压缩
spider.use_gzip=false
#爬虫存储根目录
spider.base_dir=C:/spider/
#爬虫提取到的数据存储目录
spider.data_store_dir=C:/spider/data/
#爬虫断点恢复数据存储目录
spider.data_resume_dir=C:/spider/temp/
#爬虫Http代理存储路径
spider.lastuse_proxy_dir=C:/spider/data/proxy/lastUse.proxy
#爬虫线程池默认空闲线程个数，默认设置为50个
spider.idle_thread_of_pool=50
#爬虫线程池最大线程数量，默认设置为300个
spider.max_thread_of_pool=100
#爬虫线程池空闲线程保持存活状态的最大时间，单位：毫秒
spider.idle_thread_keep_alive_time=120000
#每个线程处理任务个数，默认设置为20个
spider.task_count_per_thread=20
#每个任务包含的待抓取的URL个数
spider.url_count_per_task=10
#爬虫最大线程并发数，默认100个
spider.max_concurrent_threads=100


#立即发送数据,没有TCP延迟
spider.socket_tcp_nodelay=true
#当调用socket.close()时,是否立即关闭底层的TCP连接,-1表示立即关闭,默认为3600,单位毫秒,即延迟3600毫秒后关闭底层的TCP连接
spider.socket_close_delay=3600
#是否允许重用Socket所绑定的本地地址
spider.socket_reuse_address=true


#定义服务器是否愿意接受请求(服务器根据请求头信息判断愿意接受此请求)
spider.http_expect_continueEnabled=false;
#是否启用空闲HTTP链接检测,空闲HTTP链接检测会导致每个HTTP	请求有30毫秒的时间损耗,一般建议不要开启
spider.http_staleConnectionCheckEnabled=false;
#是否启用自动重定向,默认是开启的,也建议开启
spider.http_redirectsEnabled=true
#是否允许相对链接的自动重定向,HTTP协议规范里规定自动重定向的URL必须是一个绝对URL,默认HttpClient是允许相对链接的自动重定向
spider.http_relativeRedirectsAllowed=true
#环形重定向（重定向到相同路径）是否被允许,建议设置为允许
spider.http_circularRedirectsAllowed=true
#定义重定向的最大数量,若你没有设置此项,HttpClient默认值为50,这里设置默认值为100
spider.http_maxRedirects=100
#是否开启自动验证(可能你访问某个资源之前,服务器会要求你先进行身份验证,比如用户登录),默认自动验证是开启的
spider.http_authenticationEnabled=true
#定义从HTTP链接池获取一个HTTP链接的超时时间,单位:毫秒,默认值为-1,即表示不限制,0表示无穷大
spider.http_connectionRequestTimeout=5000
#定义建立与服务器的HTTP链接的超时时间,单位:毫秒,默认值为-1,即表示不限制,0表示无穷大
spider.http_connectTimeout=3000;
#HTTP请求默认编码UTF-8
spider.http_connect_charset=UTF-8
#连接池每个路由最大链接并发数
spider.max_pre_route=5000
#连接池HTTP请求80端口最大链接并发数
spider.max_http_route=500
#是否开启单Cookie请求头,默认开启,也建议开启
spider.http.protocol.single-cookie-header=true

#爬虫嵌入式数据库存储目录
db.berkeley_basepath=C:/berkekeyDB/
#爬虫嵌入式数据库名称
db.database_name=spider4j_db
#是否开启爬虫嵌入式数据库事务支持,默认开启
db.enable_transaction=true
#是否开启爬虫嵌入式数据库的磁盘IO延迟写入功能,默认关闭
db.enable_defer_write=false
#是否允许自动创建数据库,若数据库文件已存在则抛异常,否则自动创建数据库,默认true为允许自动创建
db.allow_auto_create_database=true
#是否开启爬虫嵌入式数据库读写锁
db.is_locking=true
#爬虫嵌入式数据库缓存字节数最大值,默认为1024 * 1024 * 4=4M
db.cache_max_size=4194304
#爬虫嵌入式数据库日志文件字节数最大值,默认为1024 * 1024 * 64=64M
db.log_max_size=67108864
#爬虫嵌入式数据库数据读写默认编码
db.default_encoding=UTF-8


#HTTP请求每个主机的最大链接数
http.max_connections_per_host=100
#HTTP请求总共最大连接数
http.max_total_connections=1000
#HTTP链接超时设置，单位毫秒，默认为30秒
http.connection_timeout=30000
#HTTP socket网络读写超时设置，单位毫秒，默认为20秒
http.socket_timeout=20000
#http代理主机ip[可选项]
http.proxy_host=xxxxx
#http代理端口号[可选项]
http.proxy_port=88888
#代理帐号
http.proxy_username=xxxxx
#代理密码
http.proxy_password=xxxxx


#数据库类型
jdbc.default_database_type=MYSQL
#主机名称或者主机IP地址
jdbc.default_host=localhost
#端口号
jdbc.default_port=3306
#数据库名称
jdbc.default_database_name=test
#登录帐号
jdbc.default_user_name=root
#登录密码
jdbc.default_password=123